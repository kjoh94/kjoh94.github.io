---
layout: post
title: "LLM과 OpenClaw는 어떻게 역할을 나눠 일하는가 (뇌 vs 손발)"
date: 2026-02-13
categories: [automation, openclaw, ai]
---

> Confluence 페이지처럼, “누가 조율하고(오케스트레이션), 누가 데이터를 수집하고, 누가 요약을 쓰는가?”를 한 장으로 정리한 글.

## 결론(한 줄)
**조율(플래닝/선택/요약)은 LLM(예: GPT-5.2)이 ‘뇌’**, 실제 데이터 수집/실행은 **OpenClaw + 로컬 스크립트가 ‘손발’** 역할을 한다.

---

## 구성요소

### A) 뇌(Brain): LLM (예: GPT-5.2)
- 사용자의 자연어 요구를 해석한다.
- “최근 N개 중 중요한 3개”, “어제/오늘만”, “링크만” 같은 요청에 맞게 **계획을 세운다.**
- 어떤 URL을 요약할지 **선택**(랭킹/필터링)한다.
- 자막 텍스트를 읽고 **요약문을 생성**한다.

### B) 조율자/런타임(Orchestrator): OpenClaw
- LLM이 세운 계획대로 **도구 실행(exec, Drive 업로드, Calendar, cron 등)** 을 실행한다.
- 정해진 시간에 자동 실행(cron)하고 결과를 사용자에게 전달할 수도 있다.

### C) 손발(Hands): 로컬 스크립트(예시 2개)
- **스크립트 A: 목록(후보) 수집**
  - 입력: (예) 플레이리스트 설정 + 최근 N개
  - 출력: JSON(기계 친화)
- **스크립트 B: 콘텐츠(자막) 수집**
  - 입력: YouTube URL
  - 출력: SRT/텍스트

핵심은, 스크립트가 “판단”을 하는 게 아니라 **사실(데이터)을 안정적으로 제공**하는 데만 집중한다는 점.

---

## ASCII 다이어그램 (전체 흐름)

```
사용자(User)
   |
   |  (자연어 요청)
   v
+-------------------+
|  LLM (GPT-5.2)    |   <- 뇌: 해석/계획/선택/요약
+-------------------+
   |
   | 1) '최근 추가 N개'가 필요함 -> OpenClaw에 exec 요청
   v
+-------------------+
| OpenClaw Runtime  |   <- 조율자: exec/drive/calendar/cron
+-------------------+
   |
   | exec: (script A) 최근 목록 JSON 출력
   v
+------------------------------+
| Script A: list items (JSON)  |   <- 손발(데이터 수집)
+------------------------------+
   |
   | JSON 결과를 LLM에게 전달
   v
+-------------------+
|  LLM (GPT-5.2)    |   <- 뇌: 필터/랭킹 ("중요한 3개" 선택)
+-------------------+
   |
   | 2) 선택된 URL들 자막 필요 -> OpenClaw에 exec 요청(반복)
   v
+-------------------+
| OpenClaw Runtime  |
+-------------------+
   |
   | exec: (script B) get subtitles
   v
+-------------------------+
| Script B: get subtitles |  <- 손발(자막 수집)
+-------------------------+
   |
   | 자막 텍스트를 LLM에게 전달
   v
+-------------------+
|  LLM (GPT-5.2)    |   <- 뇌: 요약 생성 + 포맷팅
+-------------------+
   |
   | (옵션) drive upload
   v
사용자에게 결과 전달 / Drive 저장
```

---

## “프롬프트가 매번 달라도” 가능한 이유
- 스크립트는 **딱 2가지 사실**만 제공한다:
  1) 최근 추가된 목록(후보)
  2) 특정 영상의 자막(콘텐츠)
- 나머지(필터 기준, 중요도 판단, 출력 포맷, 요약 길이)는 **LLM이 사용자 의도를 읽고 그때그때 결정**한다.

즉, “뇌”는 항상 같은 2개의 센서(목록/자막)를 들고, 매번 다른 지시(프롬프트)에 맞게 행동을 바꾸는 구조.

---

## 운영할 때의 현실적인 제약
자막을 연속으로 많이 가져오면(대량 배치) 플랫폼에서 **차단(IpBlocked)** 이 걸릴 수 있다.

실무적으로는:
- 배치로 나눠 실행(예: 5~8개씩)
- 실패 시 스킵/재시도
- 자막 실패는 제목 기반 요약으로 degrade
같은 운영 규칙을 오케스트레이터(OpenClaw) + LLM 조합으로 넣는 게 안전하다.

---

## 요약
- **뇌:** LLM(GPT-5.2) — 해석/계획/선택/요약
- **손발:** 로컬 스크립트 — 목록/자막 등 데이터 수집
- **조율자:** OpenClaw — 실행/저장/스케줄링
